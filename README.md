# ğŸš€ ELEVATELABS - Data Analysis Learning Journey

This repository contains my daily tasks and projects as part of my data analysis and visualization learning path with ElevateLabs.

---

## ğŸ¬ Movie Dataset Cleaning - Task 1

This project demonstrates **data cleaning techniques** using Python and Pandas on a movie dataset.
It is part of my learning journey to improve my **data analysis and machine learning skills**.

---

### ğŸ“Œ Project Overview

In this task, I took a raw movie dataset and cleaned it step by step.
The main goals were to:

- Handle **missing values**
- Remove **duplicate records**
- Standardize column names and data formats
- Make the dataset **ready for analysis or visualization**

---

### ğŸ› ï¸ Tools & Libraries Used

- **Python 3**
- **Pandas** â€“ for data manipulation
- **NumPy** â€“ for numerical operations



Install dependencies:
```bash
pip install pandas numpy
```

ğŸ§¾ Steps Performed
1ï¸âƒ£ Importing Libraries and Dataset

```bash
import pandas as pd
import numpy as np

# Load the dataset (replace with your file path)
df = pd.read_csv('netflix_titles.csv')
print(df.head())
```

2ï¸âƒ£ Checking for Missing Values

```bash
print("Missing Values Before Cleaning:")
print(df.isnull().sum())
```

3ï¸âƒ£ Removing Duplicate Records

```bash
df_cleaned = df.drop_duplicates()
```

4ï¸âƒ£ Standardizing Data

```bash
# Convert country names to lowercase
df_cleaned['country'] = df_cleaned['country'].str.lower()

# Convert date formats to consistent type
df_cleaned['date_added'] = pd.to_datetime(df_cleaned['date_added'], errors='coerce', format='%B %d, %Y')

# Rename column headers (lowercase, replace spaces with underscores)
df_cleaned.columns = df_cleaned.columns.str.strip().str.lower().str.replace(' ', '_')
```

5ï¸âƒ£ Fixing Data Types

```bash
df_cleaned['release_year'] = df_cleaned['release_year'].astype('Int64')
```

6ï¸âƒ£ Handling Missing Values

```bash
df_cleaned['country'] = df_cleaned['country'].fillna('unknown')
df_cleaned['director'] = df_cleaned['director'].fillna('unknown')
df_cleaned['cast'] = df_cleaned['cast'].fillna('unknown')
df_cleaned['rating'] = df_cleaned['rating'].fillna('Not Rated')
df_cleaned['duration'] = df_cleaned['duration'].fillna('unknown')
```

7ï¸âƒ£ Final Dataset Check and Save

```bash
print("\nMissing Values After Cleaning:")
print(df_cleaned.isnull().sum())

print("\nCleaned Dataset Info:")
print(df_cleaned.info())

# Save cleaned dataset (optional)
output_path = "netflix_titles_cleaned.csv"
df_cleaned.to_csv(output_path, index=False)
print(f"\nâœ… Cleaned dataset saved as: {output_path}")
```

ğŸ“‚ Project Structure

```bash
Netflix-Data-Cleaning/
â”œâ”€â”€ netflix_titles.csv              # Original raw dataset
â”œâ”€â”€ Task1.py                        # The Python data cleaning script
â””â”€â”€ netflix_titles_cleaned.csv      # The final cleaned dataset
```

---
---

## ğŸ“Š Superstore Sales Analysis - Task 2

This task focuses on creating interactive data visualizations using Tableau to analyze sales performance and derive business insights from a superstore dataset.

---

### ğŸ“Œ Project Overview
In this task, I created comprehensive dashboards and visualizations to:

Analyze sales trends across different regions
Identify top-performing product categories
Understand customer segments and their buying patterns
Provide actionable insights for business decisions

---

### ğŸ› ï¸ Tools & Technologies Used

Tableau Desktop â€“ for interactive data visualization
Microsoft Excel â€“ for initial data exploration
PDF Documentation â€“ for detailed analysis methodology

---

### ğŸ“ Files Included

1)Book.pdf

Comprehensive documentation of the analysis process
Includes methodology, insights, and recommendations

2)Book.twbx

Interactive Tableau workbook with multiple dashboards
Contains various visualizations including:
Sales by Region
Product Category Performance
Time Series Analysis
Customer Segment Analysis

3)Sample - Superstore (1).xls

Raw dataset containing 4 years of sales data
Includes information on orders, products, customers, and regions

---

### ğŸ¯ Key Insights Discovered
Regional Performance: Identified top-performing regions and underperforming areas
Product Analysis: Found which product categories drive the most profit
Seasonal Trends: Discovered patterns in sales throughout the year
Customer Segments: Analyzed which customer types contribute most to revenue

---

### ğŸ’¡ Business Recommendations
Based on the analysis, I provided recommendations for:

Inventory management optimization
Regional marketing strategies
Product portfolio decisions
Customer retention strategies

---

### ğŸš€ How to View the Analysis

1)Excel Data: Open Sample - Superstore (1).xls to explore the raw data

2)Tableau Dashboards: Open Book.twbx in Tableau Desktop/Reader

3)Documentation: Read Book.pdf for detailed insights and methodology

---

### ğŸ“¸ Dashboard Preview

The Tableau workbook includes:

Executive Summary Dashboard
Regional Performance Analysis
Product Category Deep Dive
Customer Segment Analysis
Time Series Trends

---

### ğŸ“‚ Project Structure

```bash
TASK 2/
â”œâ”€â”€ Book.pdf                        # Analysis documentation
â”œâ”€â”€ Book.twbx                       # Tableau workbook with dashboards
â”œâ”€â”€ Sample - Superstore (1).xls     # Raw superstore sales data
â””â”€â”€ README.md                       # This file
```


---
---

## ğŸ“ˆ Financial Sales Analysis - Task 3

This task focuses on analyzing **financial sales data** using Tableau and presenting insights through interactive dashboards and reports.

---

### ğŸ“Œ Project Overview

In this task, I worked on a coffee shop sales dataset and performed:

- Data preparation and cleaning
- Trend analysis on daily, weekly, and monthly sales
- Customer segmentation and product performance insights
- Creation of interactive dashboards for business decision-making

---

### ğŸ› ï¸ Tools & Technologies Used

- **Tableau Desktop** â€“ for building dashboards and interactive visualizations  
- **Microsoft Excel / CSV** â€“ for data validation and preprocessing  
- **PDF Documentation** â€“ summarizing the methodology and insights  

---

### ğŸ“ Files Included

1. **Financial Sales Analysis.pdf**  
   - Detailed documentation of methodology, KPIs, and insights

2. **Financial Sales Analysis.twbx**  
   - Interactive Tableau workbook containing:
     - Sales Trends Dashboard  
     - Category & Product Performance Analysis  
     - Regional/Store-Level Breakdown  
     - Customer Insights  

3. **dataset/Coffe_sales.csv**  
   - The raw sales dataset used for analysis  

4. **~Financial Sales Analysis__2212.twbr**  
   - Tableau backup/temporary file (useful for recovery)

---

### ğŸ¯ Key Insights Discovered

- **Sales Trends:** Identified peak hours, days, and months for maximum revenue  
- **Product Analysis:** Highlighted top-performing coffee products and low-performing items  
- **Customer Behavior:** Segmented customers by frequency and spend  
- **Revenue Optimization:** Suggested focus on high-profit categories and seasonal promotions  

---

### ğŸ’¡ Business Recommendations

Based on the insights, I recommended:

- **Stock Optimization:** Increase inventory for high-demand periods  
- **Targeted Campaigns:** Run promotions during low-performing months  
- **Menu Adjustments:** Focus marketing on high-profit products  
- **Customer Loyalty Programs:** Reward frequent buyers to improve retention  

---

### ğŸš€ How to View the Analysis

1. **Dataset:** Open `dataset/Coffe_sales.csv` to explore raw data  
2. **Tableau Dashboard:** Open `Financial Sales Analysis.twbx` in Tableau  
3. **Documentation:** Review `Financial Sales Analysis.pdf` for detailed insights  

---

### ğŸ“‚ Project Structure

```bash
TASK 3/
â”œâ”€â”€ Financial Sales Analysis.pdf          # Detailed analysis documentation
â”œâ”€â”€ Financial Sales Analysis.twbx         # Tableau workbook with dashboards
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ Coffe_sales.csv                   # Raw coffee sales dataset
â””â”€â”€ ~Financial Sales Analysis__2212.twbr  # Tableau backup file
```


---
---
## ğŸ“ˆ Financial Sales Analysis with SQLite - Task 4

This task demonstrates **data analysis using SQLite** on a financial sales dataset.  
It focuses on querying, joining tables, aggregating data, and creating views for **insightful business analysis**.

---

### ğŸ“Œ Project Overview

In this task, I worked on sales data of multiple regions and products to:

- Import and clean CSV datasets into SQLite  
- Perform **SQL queries** including selection, filtering, aggregation, and joins  
- Compare **regional performance** versus sales targets  
- Create **views for analysis** and generate outputs for reporting  

This project strengthens **SQL querying skills**, understanding of **relational databases**, and **data-driven decision making**.

---

### ğŸ› ï¸ Tools & Technologies Used

- **SQLite 3** â€“ for database management and queries  
- **CSV Datasets** â€“ raw sales data, order details, and sales targets  
- **VS Code / SQLite CLI** â€“ to run queries and manage the database  
- **Output Files** â€“ SQL query outputs and screenshots  

---

ğŸ§¾ Steps Performed  

1ï¸âƒ£ **Database Setup**

```sql
-- Create tables
CREATE TABLE orders (
    OrderID TEXT,
    OrderDate TEXT,
    CustomerName TEXT,
    Region TEXT,
    City TEXT
);

CREATE TABLE order_details (
    OrderID TEXT,
    ProductName TEXT,
    Quantity INTEGER,
    Sales REAL
);

CREATE TABLE sales_target (
    Region TEXT,
    Target REAL
);

-- Import CSV data
.mode csv
.import "List of Orders.csv" orders
.import "Order Details.csv" order_details
.import "Sales target.csv" sales_target
```
```sql
2ï¸âƒ£ Basic Queries & Filtering

sql
Copy code
-- Select first 10 orders
SELECT * FROM orders LIMIT 10;

-- Orders with above-average sales
SELECT * FROM orders
WHERE OrderID IN (
    SELECT OrderID FROM order_details
    WHERE Sales > (SELECT AVG(Sales) FROM order_details)
);
```
```sql
3ï¸âƒ£ Aggregation & Grouping

sql
Copy code
-- Total sales by region
SELECT o.Region, SUM(od.Sales) AS TotalSales
FROM orders o
JOIN order_details od ON o.OrderID = od.OrderID
GROUP BY o.Region
ORDER BY TotalSales DESC;
```
```sql
4ï¸âƒ£ Joins

sql
Copy code
-- Join orders and order_details
SELECT o.OrderID, o.Region, od.ProductName, od.Quantity, od.Sales
FROM orders o
INNER JOIN order_details od ON o.OrderID = od.OrderID
LIMIT 10;

-- Left join with sales_target
SELECT o.Region, SUM(od.Sales) AS TotalSales, st.Target
FROM orders o
LEFT JOIN order_details od ON o.OrderID = od.OrderID
LEFT JOIN sales_target st ON o.Region = st.Region
GROUP BY o.Region;
```
```sql
5ï¸âƒ£ Creating Views

sql
Copy code
-- View for regional performance
CREATE VIEW RegionalPerformance AS
SELECT o.Region, SUM(od.Sales) AS TotalSales, st.Target,
       (SUM(od.Sales) - st.Target) AS Difference
FROM orders o
JOIN order_details od ON o.OrderID = od.OrderID
LEFT JOIN sales_target st ON o.Region = st.Region
GROUP BY o.Region;

-- Query the view
SELECT * FROM RegionalPerformance
ORDER BY Difference DESC;
```
```sql
6ï¸âƒ£ Optimization

-- Indexes for faster queries
CREATE INDEX idx_orders_orderid ON orders(OrderID);
CREATE INDEX idx_orderdetails_orderid ON order_details(OrderID);
CREATE INDEX idx_orders_region ON orders(Region);
```
---
ğŸ¯ Key Insights
Regions with highest and lowest sales

Products contributing most to revenue

Regions meeting or missing sales targets

Summary view (RegionalPerformance) provides quick decision-making insights

---
ğŸ’¡ Business Recommendations
Focus marketing and promotions on low-performing regions

Maintain inventory for high-demand products

Track performance against targets regularly for better planning

Use the SQL workflow as a repeatable analysis pipeline

---
ğŸš€ How to Run Analysis
Open VS Code Terminal or Command Prompt

Navigate to the dataset folder:

Start SQLite and create the database:

```sql
sqlite3 sales.db
Import CSVs and run the SQL script:

.mode csv
.import "List of Orders.csv" orders
.import "Order Details.csv" order_details
.import "Sales target.csv" sales_target

.read output.sql

```
Output will be saved in output.txt (all queries and results)

ğŸ“‚ Project Structure
```bash
TASK 4/
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ List of Orders.csv
â”‚   â”œâ”€â”€ Order Details.csv
â”‚   â”œâ”€â”€ Sales target.csv
â”‚   â””â”€â”€ sales.db
â”œâ”€â”€ Output/
â”‚   â”œâ”€â”€ output.sql          # SQL queries
â”‚   â”œâ”€â”€ output.txt          # Query results
â”‚   â””â”€â”€ Screenshots/        # Screenshots of terminal outputs
â””â”€â”€ README.md               # Project documentation
```

---
---
## ğŸ“Š Exploratory Data Analysis (EDA) - Task 5

This task demonstrates **Exploratory Data Analysis (EDA)** using Python and Pandas on the **Titanic dataset**.  
It is part of my learning journey to strengthen **data analysis, visualization, and insight generation skills**.

---

### ğŸ“Œ Project Overview

In this task, I performed a comprehensive analysis of the Titanic dataset to uncover patterns and relationships in passenger survival.  
The main goals were to:

- Understand **dataset structure and features**
- Identify **missing values** and data quality issues
- Explore **numerical and categorical distributions**
- Visualize patterns in **survival by gender, class, age, and fare**
- Summarize key **insights for decision-making**

---

### ğŸ› ï¸ Tools & Libraries Used

- **Python 3**  
- **Pandas** â€“ for data manipulation  
- **Matplotlib & Seaborn** â€“ for data visualization  

Install dependencies:

```bash
pip install pandas matplotlib seaborn
```

ğŸ§¾ Steps Performed
1ï¸âƒ£ Importing Libraries and Dataset
```bash
from google.colab import files
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Upload Titanic train dataset
uploaded = files.upload()

# Load dataset
df = pd.read_csv("train.csv")
df.head()
```

2ï¸âƒ£ Basic Dataset Info
```bash
df.info()
```

3ï¸âƒ£ Summary Statistics
```bash
df.describe()
```

4ï¸âƒ£ Check Missing Values
```bash
df.isnull().sum()
```

5ï¸âƒ£ Count Unique Values
```bash
df.nunique()
```

6ï¸âƒ£ Age Distribution
```bash
sns.histplot(df['Age'].dropna(), kde=True)
plt.title("Age Distribution")
plt.show()
```
Observation: Most passengers are between 20â€“40 years old.

7ï¸âƒ£ Survival Count
```bash
sns.countplot(x='Survived', data=df)
plt.title("Survival Count (0 = No, 1 = Yes)")
plt.show()
```
Observation: More passengers did not survive than survived.

8ï¸âƒ£ Survival by Gender
```bash
sns.countplot(x='Sex', hue='Survived', data=df)
plt.title("Survival by Gender")
plt.show()
```
Observation: Females had a much higher survival rate than males.

9ï¸âƒ£ Survival by Passenger Class
```bash
sns.countplot(x='Pclass', hue='Survived', data=df)
plt.title("Survival by Passenger Class")
plt.show()
```
Observation: 1st class passengers survived more than 3rd class.

ğŸ”Ÿ Age vs Fare (Survival)
```bash
sns.scatterplot(x='Age', y='Fare', hue='Survived', data=df)
plt.title("Age vs Fare (colored by Survival)")
plt.show()
```
Observation: Wealthier passengers (higher Fare) had better survival chances.

1ï¸âƒ£1ï¸âƒ£ Boxplot: Age vs Survival
```bash
sns.boxplot(x='Survived', y='Age', data=df)
plt.title("Age vs Survival")
plt.show()
```
Observation: Younger passengers show slightly better survival trends.

1ï¸âƒ£2ï¸âƒ£ Correlation Heatmap
```bash
df_encoded = df.copy()
df_encoded['Sex'] = df_encoded['Sex'].map({'male': 0, 'female': 1})

plt.figure(figsize=(8,6))
sns.heatmap(df_encoded.corr(), annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()
```
Observation: Gender and class strongly correlate with survival.

1ï¸âƒ£3ï¸âƒ£ Pairplot (Optional)
```bash
sns.pairplot(df_encoded[['Survived','Age','Fare','Pclass','Sex']], hue='Survived')
plt.show()
```
1ï¸âƒ£4ï¸âƒ£ Final Summary

---
Markdown Observation:
* Survival strongly depended on gender and passenger class.

* Females had higher survival rates than males.

* 1st class passengers survived more than 3rd class.

* Younger passengers had slightly better chances.

* Higher fares correlated with survival.


ğŸŸ¢ Deliverables

---
* Notebook: .ipynb file with all code and observations

* PDF Report: Use File â†’ Print â†’ Save as PDF in Colab
---

ğŸ“‚ Project Structure
```bash
TASK 5 - Titanic EDA/
â”œâ”€â”€ train.csv                 # Raw Titanic dataset
â”œâ”€â”€ Titanic_EDA.ipynb         # Colab notebook with EDA steps
â””â”€â”€ README.md                 # Project documentation
```

---
---

## ğŸ“Š Sales Trend Analysis - Task 6

This task demonstrates **sales trend analysis using Python and SQLite**, aimed at uncovering insights from an online sales dataset and performing data cleansing, aggregation, and visualization.  

---

### ğŸ“Œ Project Overview

In this task, I analyzed online sales data to:

- Import and clean CSV datasets into SQLite  
- Perform **data validation and cleansing**  
- Analyze **sales trends by date, product category, and region**  
- Generate **summary statistics and visualizations** for business insights  

This task strengthens skills in **SQL querying, Python data analysis, and visualization**.  

---

### ğŸ› ï¸ Tools & Libraries Used

- **Python 3** â€“ for data manipulation and analysis  
- **Pandas & NumPy** â€“ for cleaning and transforming data  
- **Matplotlib & Seaborn** â€“ for visualizations  
- **SQLite 3** â€“ for database operations  
- **VS Code / SQLite CLI** â€“ to run scripts and queries  

Install Python dependencies:
```bash
pip install pandas numpy matplotlib seaborn
ğŸ§¾ Steps Performed
1ï¸âƒ£ Database Setup & CSV Import

```bash

-- Create sales table
CREATE TABLE online_sales (
    transaction_id INTEGER,
    order_date TEXT,
    product_category TEXT,
    product_name TEXT,
    amount REAL
);

-- Import CSV into SQLite
.mode csv
.import "online_sales.csv" online_sales
```

2ï¸âƒ£ Data Cleaning
```bash
python

import pandas as pd
import sqlite3

# Connect to SQLite database
conn = sqlite3.connect('sales.db')
df = pd.read_sql_query("SELECT * FROM online_sales", conn)

# Check missing values
print(df.isnull().sum())

# Fill missing values
df['amount'] = df['amount'].fillna(0)
df['product_category'] = df['product_category'].fillna('Unknown')
df['product_name'] = df['product_name'].fillna('Unknown')
```

3ï¸âƒ£ Aggregations & Trend Analysis
```bash
python

# Total sales by category
category_sales = df.groupby('product_category')['amount'].sum().reset_index()

# Sales trend over time
df['order_date'] = pd.to_datetime(df['order_date'])
daily_sales = df.groupby('order_date')['amount'].sum().reset_index()

# Plot daily sales trend
import matplotlib.pyplot as plt
plt.figure(figsize=(10,5))
plt.plot(daily_sales['order_date'], daily_sales['amount'], marker='o')
plt.title("Daily Sales Trend")
plt.xlabel("Date")
plt.ylabel("Total Sales")
plt.xticks(rotation=45)
plt.show()

```
---
ğŸ¯ Key Insights Discovered
Identified top-selling product categories

Found peak sales periods by date

Discovered regions with highest and lowest sales

Highlighted trends useful for inventory and marketing decisions

---

ğŸ’¡ Business Recommendations
Focus marketing campaigns on top-performing products

Adjust inventory according to peak sales periods

Target underperforming regions with promotions

Use this analysis workflow for future sales tracking


ğŸ“‚ Project Structure
```bash
TASK 6/
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ online_sales.csv         # Raw sales dataset
â”œâ”€â”€ Task6.py                     # Python script for analysis
â”œâ”€â”€ sales.db                     # SQLite database (after import)
â””â”€â”€ README.md                    # Project documentation
```
